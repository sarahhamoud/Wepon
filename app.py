import streamlit as st
from ultralytics import YOLO
from pathlib import Path
import tempfile
from PIL import Image
import cv2
import numpy as np
import os

if "logged_in" not in st.session_state or not st.session_state.logged_in:
    st.error("ÙŠØ¬Ø¨ ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„ Ù…Ù† Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ø£ÙˆÙ„Ø§Ù‹.")
    st.stop()

# ---------------- Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØµÙØ­Ø© ----------------
st.set_page_config(
    page_title="ØªØ·Ø¨ÙŠÙ‚ ÙƒØ´Ù Ø§Ù„Ø£Ø³Ù„Ø­Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… YOLO",
    layout="wide"
)

# âœ… ØªÙØ¹ÙŠÙ„ Ø§ØªØ¬Ø§Ù‡ Ù…Ù† Ø§Ù„ÙŠÙ…ÙŠÙ† Ù„Ù„ÙŠØ³Ø§Ø± + Ù…Ø­Ø§Ø°Ø§Ø© Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ø§Ù„ÙŠÙ…ÙŠÙ†
st.markdown(
    """
    <style>
    html, body, [data-testid="stApp"] {
        direction: RTL;
        text-align: right;
    }
    h1, h2, h3, h4, h5, h6, p, label, span, div {
        text-align: right;
    }
    </style>
    """,
    unsafe_allow_html=True
)

st.title("ğŸ”« ØªØ·Ø¨ÙŠÙ‚ ÙƒØ´Ù Ø§Ù„Ø£Ø³Ù„Ø­Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… YOLO")
st.write("Ø§Ø®ØªØ§Ø±ÙŠ Ø£Ø­Ø¯ Ø§Ù„Ø£ÙˆØ¶Ø§Ø¹ Ø§Ù„Ø«Ù„Ø§Ø«Ø©: ØµÙˆØ±Ø©ØŒ ÙÙŠØ¯ÙŠÙˆØŒ Ø£Ùˆ ÙƒØ§Ù…ÙŠØ±Ø§ Ø§Ù„Ø­Ø§Ø³ÙˆØ¨.")

# ---------------- ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© ÙÙ‚Ø· ----------------
MODEL_PATH = r"C:\Users\hp\Desktop\YOLO_project_backup\train2\weights\best.pt"

@st.cache_resource
def load_model(path):
    model = YOLO(path)
    return model

model = load_model(MODEL_PATH)

# âœ… Ù…Ø¬Ù„Ø¯ Ø­ÙØ¸ Ø§Ù„ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
OUTPUT_DIR = r"C:\Users\hp\Desktop\YOLO_project_backup\runs\streamlit\video_inference"
os.makedirs(OUTPUT_DIR, exist_ok=True)  # ÙŠØªØ£ÙƒØ¯ Ø£Ù† Ø§Ù„Ù…Ø¬Ù„Ø¯ Ù…ÙˆØ¬ÙˆØ¯

# ---------------- Ø¯Ø§Ù„Ø© ØªØ±Ø³Ù… Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø±Ø¨Ø¹Ø§Øª ----------------
def draw_boxes(frame, results):
    """
    ØªØ±Ø³Ù… ÙƒÙ„ Ø§Ù„Ù€ bounding boxes Ø¹Ù„Ù‰ Ø§Ù„ÙØ±ÙŠÙ….
    frame: ØµÙˆØ±Ø© BGR (Ù…Ù† OpenCV)
    results: Ù†Ø§ØªØ¬ model(...)
    """
    for r in results:
        boxes = r.boxes
        if boxes is None or len(boxes) == 0:
            continue

        for box in boxes:
            # Ø¥Ø­Ø¯Ø§Ø«ÙŠØ§Øª Ø§Ù„ØµÙ†Ø¯ÙˆÙ‚
            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy().astype(int)

            # Ù†Ø³Ø¨Ø© Ø§Ù„Ø«Ù‚Ø© ÙˆØ§Ù„ÙØ¦Ø©
            conf = float(box.conf[0].cpu().numpy())
            cls = int(box.cls[0].cpu().numpy())
            label = f"{model.names.get(cls, 'obj')} {conf:.2f}"

            # Ø±Ø³Ù… Ø§Ù„Ù…Ø³ØªØ·ÙŠÙ„ + Ø§Ù„Ù†Øµ
            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
            cv2.putText(frame, label, (x1, y1 - 10),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)

    return frame

# ---------------- Ø¯Ø§Ù„Ø© Ù…Ø¹Ø§Ù„Ø¬Ø© ÙÙŠØ¯ÙŠÙˆ ÙƒØ§Ù…Ù„ ÙˆØ¥Ø®Ø±Ø§Ø¬ ÙÙŠØ¯ÙŠÙˆ Ø¬Ø¯ÙŠØ¯ ----------------
def process_video(input_path, output_path):
    cap = cv2.VideoCapture(input_path)
    if not cap.isOpened():
        raise RuntimeError("âŒ Ù„Ø§ ÙŠÙ…ÙƒÙ† ÙØªØ­ Ù…Ù„Ù Ø§Ù„ÙÙŠØ¯ÙŠÙˆ")

    fps = cap.get(cv2.CAP_PROP_FPS) or 25
    width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

    fourcc = cv2.VideoWriter_fourcc(*"mp4v")
    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))

    frame_count = 0

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        frame_count += 1

        # ğŸ”¥ ØªØ·Ø¨ÙŠÙ‚ YOLO Ø¹Ù„Ù‰ ÙƒÙ„ ÙØ±ÙŠÙ…
        results = model(
            frame,
            imgsz=640,
            conf=0.20,  # Ø£Ù‚Ù„ Ø´ÙˆÙŠØ© Ø¹Ø´Ø§Ù† ÙŠÙƒØ´Ù Ø£Ø³Ù„Ø­Ø© Ø£ÙƒØ«Ø±
            iou=0.40    # ÙŠÙ‚Ù„Ù„ Ø¯Ù…Ø¬ Ø§Ù„ØµÙ†Ø§Ø¯ÙŠÙ‚ Ø§Ù„Ù‚Ø±ÙŠØ¨Ø©
        )

        annotated_frame = draw_boxes(frame, results)
        out.write(annotated_frame)

    cap.release()
    out.release()

# ---------------- ÙˆØ¶Ø¹ Ø§Ù„ØµÙˆØ±Ø© ----------------
def run_image_mode():
    st.subheader("ğŸ“· ÙˆØ¶Ø¹ Ø§Ù„ØµÙˆØ±Ø©")
    uploaded_file = st.file_uploader("Ø§Ø±ÙØ¹ÙŠ ØµÙˆØ±Ø©", type=["jpg", "jpeg", "png"])

    if uploaded_file is not None:
        image = Image.open(uploaded_file).convert("RGB")
        st.image(image, caption="Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ©", use_container_width=True)

        with st.spinner("Ø¬Ø§Ø±ÙŠ Ø§Ù„ÙƒØ´Ù Ø¹Ù† Ø§Ù„Ø£Ø³Ù„Ø­Ø© ÙÙŠ Ø§Ù„ØµÙˆØ±Ø©..."):
            # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© Ù„Ù€ NumPy Ø«Ù… BGR
            img_np = np.array(image)
            img_bgr = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)

            # ğŸ”¥ Ù†Ø³ØªØ®Ø¯Ù… model(...) Ù…Ø¹ conf Ùˆ iou Ø£Ù‚Ù„ Ø´ÙˆÙŠØ©
            results = model(
                img_bgr,
                imgsz=640,
                conf=0.20,
                iou=0.40
            )

            annotated_bgr = draw_boxes(img_bgr.copy(), results)
            annotated_rgb = cv2.cvtColor(annotated_bgr, cv2.COLOR_BGR2RGB)

        st.success("ØªÙ…Øª Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ÙƒØ´Ù âœ…")
        st.image(annotated_rgb, caption="Ù†ØªÙŠØ¬Ø© Ø§Ù„ÙƒØ´Ù (ÙƒÙ„ Ø§Ù„Ù…Ø±Ø¨Ø¹Ø§Øª)", use_container_width=True)

# ---------------- ÙˆØ¶Ø¹ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ ----------------
def run_video_mode():
    st.subheader("ğŸï¸ ÙˆØ¶Ø¹ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ")
    video_file = st.file_uploader("Ø§Ø±ÙØ¹ÙŠ ÙÙŠØ¯ÙŠÙˆ", type=["mp4", "avi", "mov", "mkv"])

    if video_file is not None:
        # Ø­ÙØ¸ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø§Ù„Ù…Ø±ÙÙˆØ¹ ÙÙŠ Ù…Ù„Ù Ù…Ø¤Ù‚Øª
        tfile = tempfile.NamedTemporaryFile(delete=False, suffix=".mp4")
        tfile.write(video_file.read())
        tfile.flush()

        # Ø¹Ø±Ø¶ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø§Ù„Ø£ØµÙ„ÙŠ
        st.video(tfile.name)

        if st.button("ğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©"):
            st.info("Ø¬Ø§Ø±ÙŠ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©...")

            # Ù…Ø³Ø§Ø± Ø­ÙØ¸ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø§Ù„Ù†Ø§ØªØ¬
            output_path = os.path.join(
                OUTPUT_DIR,
                Path(video_file.name).stem + "_processed.mp4"
            )

            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙÙŠØ¯ÙŠÙˆ
            process_video(tfile.name, output_path)

            st.success("ØªÙ…Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø¬Ø§Ø­!")


            # Ø¹Ø±Ø¶ Ø§Ù„Ù…Ø³Ø§Ø±
            st.write("Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬")
            st.code(output_path)

            # Ø²Ø± ØªØ­Ù…ÙŠÙ„
            with open(output_path, "rb") as f:
                st.download_button(
                    label="â¬‡ï¸ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ø¹Ø¯ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©",
                    data=f.read(),
                    file_name="processed_video.mp4",
                    mime="video/mp4"
                )


# ---------------- ÙˆØ¶Ø¹ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§ ----------------
def run_camera_mode():
    st.subheader("ğŸ“¸ ÙˆØ¶Ø¹ ÙƒØ§Ù…ÙŠØ±Ø§ Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ (ØµÙˆØ±Ø© ÙˆØ§Ø­Ø¯Ø©)")
    st.write("Ø§Ø¶ØºØ·ÙŠ Ø¹Ù„Ù‰ Ø²Ø± Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§ Ù„Ø§Ù„ØªÙ‚Ø§Ø· ØµÙˆØ±Ø© Ù…Ù† ÙƒØ§Ù…ÙŠØ±Ø§ Ø§Ù„Ù„Ø§Ø¨ØªÙˆØ¨ØŒ Ø«Ù… Ø³Ø£Ø·Ø¨Ù‚ Ø¹Ù„ÙŠÙ‡Ø§ Ù†Ù…ÙˆØ°Ø¬ YOLO.")

    img_data = st.camera_input("Ø§Ù„ØªÙ‚Ø·ÙŠ ØµÙˆØ±Ø© Ù…Ù† Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§")

    if img_data is not None:
        image = Image.open(img_data).convert("RGB")
        st.image(image, caption="Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…Ù„ØªÙ‚Ø·Ø©", use_container_width=True)

        with st.spinner("Ø¬Ø§Ø±ÙŠ Ø§Ù„ÙƒØ´Ù Ø¹Ù† Ø§Ù„Ø£Ø³Ù„Ø­Ø© ÙÙŠ Ø§Ù„ØµÙˆØ±Ø©..."):
            img_np = np.array(image)
            img_bgr = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)

            results = model(
                img_bgr,
                imgsz=640,
                conf=0.20,
                iou=0.40
            )

            annotated_bgr = draw_boxes(img_bgr.copy(), results)
            annotated_rgb = cv2.cvtColor(annotated_bgr, cv2.COLOR_BGR2RGB)

        st.success("ØªÙ…Øª Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ÙƒØ´Ù âœ…")
        st.image(annotated_rgb, caption="Ù†ØªÙŠØ¬Ø© Ø§Ù„ÙƒØ´Ù (ÙƒÙ„ Ø§Ù„Ù…Ø±Ø¨Ø¹Ø§Øª)", use_container_width=True)

# ---------------- ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø£ÙˆØ¶Ø§Ø¹ Ø§Ù„Ø«Ù„Ø§Ø«Ø© ----------------
if "mode" not in st.session_state:
    st.session_state["mode"] = "image"

cols = st.columns(3)

with cols[0]:
    if st.button("ğŸ–¼ï¸ ÙƒØ´Ù Ù…Ù† ØµÙˆØ±Ø©", use_container_width=True):
        st.session_state["mode"] = "image"

with cols[1]:
    if st.button("ğŸ¬ ÙƒØ´Ù Ù…Ù† ÙÙŠØ¯ÙŠÙˆ", use_container_width=True):
        st.session_state["mode"] = "video"

with cols[2]:
    if st.button("ğŸ“¹ ÙƒØ´Ù Ù…Ù† ÙƒØ§Ù…ÙŠØ±Ø§ Ø§Ù„Ø­Ø§Ø³ÙˆØ¨", use_container_width=True):
        st.session_state["mode"] = "camera"

st.markdown("---")

mode = st.session_state["mode"]

if mode == "image":
    run_image_mode()
elif mode == "video":
    run_video_mode()
elif mode == "camera":
    run_camera_mode()
